# ğŸ›’ Segmentation Client : Analyse de Clustering K-Means sur le Comportement Consommateur
<img src="WhatsApp Image 2025-12-04 Ã  11.45.28_0da4a02f.jpg" style="height:264px;margin-right:232px"/>
**Ezraidy soulaimane**  
**Projet Data Science & Machine Learning**  
**AnnÃ©e Universitaire 2025-2026**  
**ThÃ©matique:** Segmentation Client (Clustering)

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Introduction](#1-introduction)
2. [Dataset et ProblÃ©matique](#2-dataset-et-problÃ©matique)
3. [MÃ©thodologie](#3-mÃ©thodologie)
4. [ImplÃ©mentation Technique](#4-implÃ©mentation-technique)
5. [RÃ©sultats et Discussion](#5-rÃ©sultats-et-discussion)
6. [Conclusion](#6-conclusion)
7. [Bibliographie](#7-bibliographie)

---

## 1. Introduction

### 1.1 Contexte

Dans un environnement commercial de plus en plus compÃ©titif, la comprÃ©hension approfondie du comportement des consommateurs est devenue un avantage stratÃ©gique majeur. Les entreprises cherchent Ã  **personnaliser leurs stratÃ©gies marketing** pour maximiser l'engagement client, amÃ©liorer la rÃ©tention et augmenter le chiffre d'affaires.

La **segmentation client** permet de regrouper les consommateurs en clusters homogÃ¨nes partageant des caractÃ©ristiques comportementales similaires. Cette approche facilite :
- ğŸ¯ Le **ciblage marketing** prÃ©cis et personnalisÃ©
- ğŸ’° L'**optimisation des ressources** publicitaires
- ğŸ“ˆ L'**amÃ©lioration de la satisfaction client** par des offres adaptÃ©es
- ğŸ”„ La **prÃ©diction du churn** et des opportunitÃ©s de fidÃ©lisation

### 1.2 ProblÃ©matique

**Question centrale :** Comment identifier et caractÃ©riser des groupes distincts de clients Ã  partir de donnÃ©es comportementales et dÃ©mographiques pour crÃ©er des stratÃ©gies marketing ciblÃ©es ?

**Objectifs spÃ©cifiques :**
1. Segmenter la base clients en groupes cohÃ©rents
2. Identifier les profils types de consommateurs
3. Fournir des insights actionnables pour les Ã©quipes marketing
4. Optimiser l'allocation des ressources commerciales

### 1.3 Type de Machine Learning

**Apprentissage non supervisÃ© (Unsupervised Learning) - Clustering**

- **Algorithme principal :** K-Means Clustering
- **Pas de variable cible** : L'objectif est de dÃ©couvrir des structures latentes dans les donnÃ©es
- **MÃ©thode de validation :** Elbow Method, Silhouette Score

---

## 2. Dataset et ProblÃ©matique

### 2.1 Source des DonnÃ©es

**Dataset :** Customer Personality Analysis  
**Origine :** Kaggle ([lien dataset](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis))  
**Format :** Fichier CSV/TSV (`marketing_campaign.csv`)  
**CrÃ©ateur :** Dr. Omar Romero-Hernandez

### 2.2 Description GÃ©nÃ©rale

Le dataset contient des donnÃ©es sur **2 240 clients** d'une entreprise de vente au dÃ©tail, collectÃ©es entre 2012 et 2014. Il comprend :
- ğŸ“Š **29 variables** initiales (rÃ©duites Ã  ~25 aprÃ¨s nettoyage)
- ğŸ§‘â€ğŸ¤â€ğŸ§‘ DonnÃ©es dÃ©mographiques (Ã¢ge, Ã©ducation, situation familiale)
- ğŸ’³ Comportements d'achat (montants dÃ©pensÃ©s par catÃ©gorie de produits)
- ğŸ“¢ RÃ©ponses aux campagnes marketing
- ğŸ›’ Canaux d'achat prÃ©fÃ©rÃ©s

### 2.3 Dictionnaire de DonnÃ©es

#### **2.3.1 Variables DÃ©mographiques**

| Variable | Type | Description | Exemple |
|----------|------|-------------|---------|
| `ID` | int | Identifiant unique client | 5524 |
| `Year_Birth` | int | AnnÃ©e de naissance | 1957 |
| `Education` | str | Niveau d'Ã©ducation | Graduation, PhD, Master, 2n Cycle, Basic |
| `Marital_Status` | str | Situation familiale | Single, Together, Married, Divorced, Widow, Alone, Absurd, YOLO |
| `Income` | float | Revenu annuel du foyer (USD) | 58138.0 |
| `Kidhome` | int | Nombre d'enfants en bas Ã¢ge | 0, 1, 2 |
| `Teenhome` | int | Nombre d'adolescents | 0, 1, 2 |
| `Dt_Customer` | date | Date d'inscription | 04-09-2012 |

#### **2.3.2 Variables de DÃ©penses (Produits)**

| Variable | Type | Description | Plage |
|----------|------|-------------|-------|
| `MntWines` | int | Montant dÃ©pensÃ© en vins (2 ans) | 0 - 1493 USD |
| `MntFruits` | int | Montant dÃ©pensÃ© en fruits | 0 - 199 USD |
| `MntMeatProducts` | int | Montant dÃ©pensÃ© en viande | 0 - 1725 USD |
| `MntFishProducts` | int | Montant dÃ©pensÃ© en poisson | 0 - 259 USD |
| `MntSweetProducts` | int | Montant dÃ©pensÃ© en sucreries | 0 - 263 USD |
| `MntGoldProds` | int | Montant dÃ©pensÃ© en produits premium | 0 - 362 USD |

#### **2.3.3 Variables Comportementales (Canaux d'Achat)**

| Variable | Type | Description |
|----------|------|-------------|
| `NumDealsPurchases` | int | Nombre d'achats avec rÃ©duction |
| `NumWebPurchases` | int | Nombre d'achats via site web |
| `NumCatalogPurchases` | int | Nombre d'achats via catalogue |
| `NumStorePurchases` | int | Nombre d'achats en magasin physique |
| `NumWebVisitsMonth` | int | Nombre de visites web/mois |

#### **2.3.4 Variables Marketing (Campagnes)**

| Variable | Type | Description |
|----------|------|-------------|
| `AcceptedCmp1` | bool | A acceptÃ© l'offre campagne 1 (0/1) |
| `AcceptedCmp2` | bool | A acceptÃ© l'offre campagne 2 (0/1) |
| `AcceptedCmp3` | bool | A acceptÃ© l'offre campagne 3 (0/1) |
| `AcceptedCmp4` | bool | A acceptÃ© l'offre campagne 4 (0/1) |
| `AcceptedCmp5` | bool | A acceptÃ© l'offre campagne 5 (0/1) |
| `Response` | bool | A acceptÃ© la derniÃ¨re campagne (0/1) |
| `Complain` | bool | A dÃ©posÃ© une plainte (2 derniÃ¨res annÃ©es) |

#### **2.3.5 Variables Ã  Exclure**

| Variable | Raison de l'exclusion |
|----------|----------------------|
| `Z_CostContact` | Constante (valeur = 3 pour tous) - Aucune variance |
| `Z_Revenue` | Constante (valeur = 11 pour tous) - Aucune variance |

### 2.4 CaractÃ©ristiques du Dataset

**Taille :**
- **Lignes :** 2 240 observations
- **Colonnes :** 29 variables initiales â†’ ~22-25 aprÃ¨s feature engineering

**Types de variables :**
- **NumÃ©riques continues :** 16 (Income, dÃ©penses, Ã¢ge calculÃ©)
- **NumÃ©riques discrÃ¨tes :** 8 (compteurs d'achats, campagnes)
- **CatÃ©gorielles :** 2 (Education, Marital_Status)
- **Date :** 1 (Dt_Customer)

**Valeurs manquantes :**
- `Income` : **24 valeurs manquantes** (~1,07%)
- Autres variables : ComplÃ¨tes

**Outliers potentiels :**
- `Year_Birth` : Clients nÃ©s en 1893, 1899, 1900 (Ã¢ges > 120 ans) â†’ Erreurs de saisie
- `Income` : Revenus > 600 000 USD (top 0,1%) â†’ Valeurs atypiques extrÃªmes

---

## 3. MÃ©thodologie

### 3.1 Pipeline Global

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DonnÃ©es Brutes â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Preprocessing   â”‚â—„â”€â”€â”€ Nettoyage, imputation, encodage
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Feature Eng.    â”‚â—„â”€â”€â”€ CrÃ©ation de features, scaling
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. EDA             â”‚â—„â”€â”€â”€ Visualisations, corrÃ©lations
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. RÃ©duction Dim.  â”‚â—„â”€â”€â”€ PCA (optionnel)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Clustering      â”‚â—„â”€â”€â”€ K-Means, dÃ©termination de k
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Validation      â”‚â—„â”€â”€â”€ Elbow, Silhouette Score
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. InterprÃ©tation  â”‚â—„â”€â”€â”€ Profils, insights business
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Justification des Choix Techniques

#### **3.2.1 Pourquoi K-Means ?**

**Avantages :**
- âœ… **SimplicitÃ© et rapiditÃ©** : Excellent sur datasets de taille moyenne (2K observations)
- âœ… **ScalabilitÃ©** : ComplexitÃ© O(nÂ·kÂ·iÂ·d) oÃ¹ i = itÃ©rations, d = dimensions
- âœ… **InterprÃ©tabilitÃ©** : Les centroÃ¯des donnent des profils moyens clairs
- âœ… **AdaptÃ© aux donnÃ©es numÃ©riques** aprÃ¨s normalisation

**Limites assumÃ©es :**
- âš ï¸ SensibilitÃ© aux outliers (d'oÃ¹ l'importance du preprocessing)
- âš ï¸ Suppose des clusters sphÃ©riques de taille similaire
- âš ï¸ NÃ©cessite de spÃ©cifier k Ã  l'avance (rÃ©solu par Elbow Method)

#### **3.2.2 Pourquoi la normalisation StandardScaler ?**

**ProblÃ¨me :** Les variables ont des Ã©chelles trÃ¨s diffÃ©rentes :
- `Income` : 0 - 600 000 USD
- `MntWines` : 0 - 1 493 USD  
- `NumWebVisitsMonth` : 0 - 20 visites

**Solution :** StandardScaler (z-score normalization)
```python
z = (x - Î¼) / Ïƒ
```
- Transforme chaque variable : **moyenne = 0, Ã©cart-type = 1**
- K-Means utilise la distance euclidienne â†’ Ã‰vite que les grandes valeurs dominent

**Alternative considÃ©rÃ©e :** MinMaxScaler (rejetÃ© car sensible aux outliers extrÃªmes)

#### **3.2.3 Pourquoi le Silhouette Score ?**

**Formule :**
```
s(i) = (b(i) - a(i)) / max(a(i), b(i))
```
OÃ¹ :
- `a(i)` = distance intra-cluster moyenne
- `b(i)` = distance inter-cluster moyenne au cluster le plus proche

**InterprÃ©tation :**
- **s(i) â‰ˆ 1** : Bien clustÃ©risÃ©
- **s(i) â‰ˆ 0** : Ã€ la frontiÃ¨re entre deux clusters
- **s(i) < 0** : Mal affectÃ© (devrait Ãªtre dans un autre cluster)

---

## 4. ImplÃ©mentation Technique

### 4.1 Environnement et BibliothÃ¨ques

```python
# Manipulation de donnÃ©es
import pandas as pd
import numpy as np

# Visualisation
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Machine Learning
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples

# Statistiques
from scipy.stats import zscore
from scipy.cluster.hierarchy import dendrogram, linkage

# Warnings
import warnings
warnings.filterwarnings('ignore')
```

### 4.2 Ã‰tape 1 : Chargement et Exploration Initiale

```python
# Chargement des donnÃ©es
df = pd.read_csv('marketing_campaign.csv', sep='\t')

# AperÃ§u des donnÃ©es
print(f"Dimensions du dataset : {df.shape}")
print(f"Nombre de lignes : {df.shape[0]}")
print(f"Nombre de colonnes : {df.shape[1]}")

# Informations sur les types de donnÃ©es
df.info()

# Statistiques descriptives
df.describe()

# VÃ©rification des valeurs manquantes
missing_values = df.isnull().sum()
print(f"\nValeurs manquantes :\n{missing_values[missing_values > 0]}")
```

**RÃ©sultat attendu :**
```
Dimensions du dataset : (2240, 29)
Valeurs manquantes :
Income    24
```

### 4.3 Ã‰tape 2 : Preprocessing (Nettoyage des DonnÃ©es)

#### **4.3.1 Suppression des Colonnes Inutiles**

```python
# Suppression des variables constantes (aucune information)
df_clean = df.drop(['Z_CostContact', 'Z_Revenue'], axis=1)
```

**Justification :** Ces colonnes ont la mÃªme valeur pour tous les clients (variance nulle).

#### **4.3.2 Gestion des Valeurs Manquantes**

```python
# Option 1 : Imputation par la mÃ©diane (robuste aux outliers)
df_clean['Income'].fillna(df_clean['Income'].median(), inplace=True)

# Option 2 (alternative) : Suppression des lignes avec NA
# df_clean = df_clean.dropna(subset=['Income'])
```

**Justification :** 
- Seulement 24 valeurs manquantes (1,07%) sur `Income`
- La mÃ©diane est plus robuste que la moyenne face aux outliers
- PrÃ©servation de 100% des donnÃ©es

#### **4.3.3 DÃ©tection et Traitement des Outliers**

```python
# DÃ©tection des Ã¢ges aberrants
df_clean['Age'] = 2022 - df_clean['Year_Birth']
print(df_clean[df_clean['Age'] > 100])

# Suppression des outliers extrÃªmes
df_clean = df_clean[df_clean['Age'] <= 100]

# Suppression des revenus extrÃªmes (> 600k USD)
df_clean = df_clean[df_clean['Income'] <= 600000]

print(f"Nouvelles dimensions : {df_clean.shape}")
```

**RÃ©sultat attendu :** ~2 200 lignes restantes

#### **4.3.4 Feature Engineering : CrÃ©ation de Nouvelles Variables**

```python
# 1. Ã‚ge du client (2022 - annÃ©e de naissance)
df_clean['Age'] = 2022 - df_clean['Year_Birth']
df_clean.drop('Year_Birth', axis=1, inplace=True)

# 2. Nombre total d'enfants
df_clean['Total_Children'] = df_clean['Kidhome'] + df_clean['Teenhome']
df_clean.drop(['Kidhome', 'Teenhome'], axis=1, inplace=True)

# 3. AnciennetÃ© client (en jours)
df_clean['Dt_Customer'] = pd.to_datetime(df_clean['Dt_Customer'], format='%d-%m-%Y')
df_clean['Days_As_Customer'] = (pd.to_datetime('2022-01-01') - df_clean['Dt_Customer']).dt.days

# 4. DÃ©penses totales
spending_cols = ['MntWines', 'MntFruits', 'MntMeatProducts', 
                 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']
df_clean['Total_Spending'] = df_clean[spending_cols].sum(axis=1)

# 5. Nombre total d'achats
purchase_cols = ['NumDealsPurchases', 'NumWebPurchases', 
                 'NumCatalogPurchases', 'NumStorePurchases']
df_clean['Total_Purchases'] = df_clean[purchase_cols].sum(axis=1)

# 6. Taux d'acceptation des campagnes
campaign_cols = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 
                 'AcceptedCmp4', 'AcceptedCmp5', 'Response']
df_clean['Total_Accepted_Campaigns'] = df_clean[campaign_cols].sum(axis=1)
df_clean['Campaign_Acceptance_Rate'] = df_clean['Total_Accepted_Campaigns'] / 6

# 7. DÃ©pense moyenne par achat
df_clean['Avg_Spending_Per_Purchase'] = df_clean['Total_Spending'] / (df_clean['Total_Purchases'] + 1)
```

**Justification :**
- **Age** : Plus interprÃ©table que l'annÃ©e de naissance
- **Total_Children** : Variable agrÃ©gÃ©e plus simple
- **Total_Spending** : Indicateur global du pouvoir d'achat
- **Campaign_Acceptance_Rate** : Mesure de l'engagement marketing

#### **4.3.5 Encodage des Variables CatÃ©gorielles**

```python
# Nettoyage de la variable Education
education_mapping = {
    'Graduation': 'Undergraduate',
    'PhD': 'Postgraduate',
    'Master': 'Postgraduate',
    '2n Cycle': 'Undergraduate',
    'Basic': 'High School'
}
df_clean['Education'] = df_clean['Education'].replace(education_mapping)

# Nettoyage de Marital_Status
marital_mapping = {
    'Married': 'In Relationship',
    'Together': 'In Relationship',
    'Single': 'Single',
    'Divorced': 'Single',
    'Widow': 'Single',
    'Alone': 'Single',
    'Absurd': 'Single',
    'YOLO': 'Single'
}
df_clean['Marital_Status'] = df_clean['Marital_Status'].replace(marital_mapping)

# Encodage One-Hot
df_encoded = pd.get_dummies(df_clean, columns=['Education', 'Marital_Status'], drop_first=True)
```

**Justification :**
- **Regroupement** : RÃ©duction de la cardinalitÃ© (8 â†’ 2 catÃ©gories pour Marital_Status)
- **One-Hot Encoding** : K-Means nÃ©cessite des donnÃ©es numÃ©riques
- **drop_first=True** : Ã‰vite la multicolinÃ©aritÃ© parfaite

### 4.4 Ã‰tape 3 : Analyse Exploratoire des DonnÃ©es (EDA)

#### **4.4.1 Distribution de l'Ã‚ge**

```python
plt.figure(figsize=(10, 5))
sns.histplot(df_clean['Age'], bins=30, kde=True, color='skyblue')
plt.title('Distribution de l\'Ã‚ge des Clients', fontsize=16, fontweight='bold')
plt.xlabel('Ã‚ge')
plt.ylabel('FrÃ©quence')
plt.axvline(df_clean['Age'].mean(), color='red', linestyle='--', label=f'Moyenne : {df_clean["Age"].mean():.1f} ans')
plt.legend()
plt.show()
```

**InterprÃ©tation :** 
- **Moyenne** : ~52 ans
- **Distribution** : Quasi-normale, lÃ©gÃ¨rement asymÃ©trique vers la droite
- **Insight** : ClientÃ¨le mature, cible marketing adaptÃ©e aux 45-65 ans

#### **4.4.2 Distribution des Revenus**

```python
plt.figure(figsize=(10, 5))
sns.boxplot(x=df_clean['Income'], color='lightgreen')
plt.title('Distribution des Revenus Annuels', fontsize=16, fontweight='bold')
plt.xlabel('Revenu (USD)')
plt.show()
```

**InterprÃ©tation :**
- **MÃ©diane** : ~51 000 USD
- **Outliers** : Quelques clients avec revenus > 150 000 USD
- **Insight** : MajoritÃ© classe moyenne, segment premium minoritaire

#### **4.4.3 Matrice de CorrÃ©lation (Heatmap)**

```python
# SÃ©lection des variables numÃ©riques
numerical_cols = df_clean.select_dtypes(include=[np.number]).columns

# Calcul de la matrice de corrÃ©lation
corr_matrix = df_clean[numerical_cols].corr()

# Visualisation
plt.figure(figsize=(18, 14))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            linewidths=0.5, cbar_kws={'label': 'Coefficient de corrÃ©lation'})
plt.title('Matrice de CorrÃ©lation des Variables NumÃ©riques', fontsize=18, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()
```

**InterprÃ©tation des corrÃ©lations clÃ©s :**

| Variables | CorrÃ©lation | Signification |
|-----------|-------------|---------------|
| `Income` â†” `Total_Spending` | **+0.78** | Fort pouvoir prÃ©dictif : revenus Ã©levÃ©s = dÃ©penses Ã©levÃ©es |
| `MntWines` â†” `MntMeatProducts` | **+0.72** | Achats liÃ©s (clients gourmets) |
| `Total_Children` â†” `Total_Spending` | **-0.42** | Familles nombreuses dÃ©pensent moins |
| `NumWebVisitsMonth` â†” `Total_Purchases` | **-0.35** | Visites frÃ©quentes mais peu d'achats = friction UX ? |

#### **4.4.4 Analyse BivariÃ©e : Revenus vs DÃ©penses**

```python
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_clean, x='Income', y='Total_Spending', 
                hue='Total_Children', palette='viridis', s=100, alpha=0.6)
plt.title('Relation Revenus - DÃ©penses Totales', fontsize=16, fontweight='bold')
plt.xlabel('Revenu Annuel (USD)')
plt.ylabel('DÃ©penses Totales (USD)')
plt.legend(title='Nombre d\'enfants', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()
```

**InterprÃ©tation :**
- **CorrÃ©lation positive forte** : Confirme la relation revenu-dÃ©pense
- **Segmentation visible** : Clients sans enfants dÃ©pensent plus Ã  revenu Ã©gal
- **Insight** : OpportunitÃ© de ciblage pour produits premium

### 4.5 Ã‰tape 4 : Normalisation des DonnÃ©es

```python
# SÃ©lection des features pour le clustering
features_for_clustering = [
    'Age', 'Income', 'Total_Children', 'Days_As_Customer',
    'Total_Spending', 'Total_Purchases', 'Avg_Spending_Per_Purchase',
    'NumWebVisitsMonth', 'Campaign_Acceptance_Rate'
]

X = df_encoded[features_for_clustering]

# Standardisation (z-score)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Conversion en DataFrame pour visualisation
X_scaled_df = pd.DataFrame(X_scaled, columns=features_for_clustering)

print("DonnÃ©es normalisÃ©es (5 premiÃ¨res lignes) :")
print(X_scaled_df.head())
```

**RÃ©sultat :** Toutes les variables ont maintenant une moyenne â‰ˆ 0 et un Ã©cart-type â‰ˆ 1.

### 4.6 Ã‰tape 5 : DÃ©termination du Nombre Optimal de Clusters (k)

#### **4.6.1 MÃ©thode du Coude (Elbow Method)**

```python
# Calcul de l'inertie (WCSS) pour k de 1 Ã  10
wcss = []
k_range = range(1, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Visualisation
plt.figure(figsize=(10, 6))
plt.plot(k_range, wcss, 'bo-', linewidth=2, markersize=10)
plt.xlabel('Nombre de Clusters (k)', fontsize=14)
plt.ylabel('WCSS (Inertie)', fontsize=14)
plt.title('MÃ©thode du Coude pour DÃ©termination de k', fontsize=16, fontweight='bold')
plt.xticks(k_range)
plt.grid(True, alpha=0.3)
plt.axvline(x=4, color='red', linestyle='--', label='Coude suggÃ©rÃ© : k=4')
plt.legend()
plt.show()
```

**InterprÃ©tation :**
- **WCSS** (Within-Cluster Sum of Squares) : Mesure la compacitÃ© intra-cluster
- **Coude** visible autour de **k = 3-4**
- Au-delÃ  de k=4, la diminution du WCSS est marginale

#### **4.6.2 Silhouette Score**

```python
silhouette_scores = []

for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    silhouette_scores.append(score)
    print(f"k = {k} : Silhouette Score = {score:.4f}")

# Visualisation
plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), silhouette_scores, 'go-', linewidth=2, markersize=10)
plt.xlabel('Nombre de Clusters (k)', fontsize=14)
plt.ylabel('Silhouette Score', fontsize=14)
plt.title('Silhouette Score en Fonction de k', fontsize=16, fontweight='bold')
plt.xticks(range(2, 11))
plt.grid(True, alpha=0.3)
plt.axhline(y=max(silhouette_scores), color='red', linestyle='--', label=f'Maximum : k={silhouette_scores.index(max(silhouette_scores))+2}')
plt.legend()
plt.show()
```

**RÃ©sultat attendu :**
```
k = 2 : Silhouette Score = 0.3812
k = 3 : Silhouette Score = 0.4051
k = 4 : Silhouette Score = 0.4127  â† Maximum
k = 5 : Silhouette Score = 0.3956
```

**DÃ©cision :** **k = 4** clusters (compromis entre Elbow et Silhouette Score maximum)

### 4.7 Ã‰tape 6 : Clustering K-Means Final

```python
# Application de K-Means avec k=4
optimal_k = 4
kmeans_final = KMeans(n_clusters=optimal_k, init='k-means++', 
                      random_state=42, n_init=20, max_iter=300)
df_encoded['Cluster'] = kmeans_final.fit_predict(X_scaled)

# Affichage de la rÃ©partition
print(df_encoded['Cluster'].value_counts().sort_index())
```

**RÃ©sultat attendu :**
```
Cluster
0    623
1    512
2    589
3    476
```

### 4.8 Ã‰tape 7 : Visualisation des Clusters

#### **4.8.1 RÃ©duction de DimensionnalitÃ© avec PCA**

```python
from sklearn.decomposition import PCA

# RÃ©duction Ã  2 dimensions pour visualisation
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Ajout au DataFrame
df_encoded['PCA1'] = X_pca[:, 0]
df_encoded['PCA2'] = X_pca[:, 1]

# Variance expliquÃ©e
print(f"Variance expliquÃ©e par PCA1 : {pca.explained_variance_ratio_[0]:.2%}")
print(f"Variance expliquÃ©e par PCA2 : {pca.explained_variance_ratio_[1]:.2%}")
print(f"Variance totale expliquÃ©e : {sum(pca.explained_variance_ratio_):.2%}")
```

**RÃ©sultat attendu :** ~60-70% de variance expliquÃ©e (suffisant pour visualisation)

#### **4.8.2 Scatter Plot 2D (PCA)**

```python
plt.figure(figsize=(12, 8))
scatter = sns.scatterplot(data=df_encoded, x='PCA1', y='PCA2', hue='Cluster', 
                          palette='Set2', s=100, alpha=0.7, edgecolor='black')
plt.title('Visualisation des Clusters (PCA 2D)', fontsize=18, fontweight='bold')
plt.xlabel(f'Composante Principale 1 ({pca
